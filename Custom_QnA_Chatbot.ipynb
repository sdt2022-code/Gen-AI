{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "r8X5o31JLYOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/requirements.txt -q"
      ],
      "metadata": {
        "id": "8UnNshPQ7yrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNM_0AoB8F6x",
        "outputId": "6141f822-948d-4622-8edb-4ee6552a7e0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/277.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/277.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.9/277.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain -q"
      ],
      "metadata": {
        "id": "rn4YLSBG8bCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.document_loaders import WikipediaLoader\n"
      ],
      "metadata": {
        "id": "2zkjZrPG8ZK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Brd8deDGzi_D",
        "outputId": "3964a7c4-f14a-41b3-8ae3-496f1b70a325"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "from dotenv import load_dotenv,find_dotenv\n",
        "load_dotenv('/content/drive/MyDrive/Environments/.env',override=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install docx2txt -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GBgOEA1_YOd",
        "outputId": "c91ddee7-84d4-4b3c-be98-df587e69af61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdeCt8pWATmW",
        "outputId": "a1c0887e-1ec0-4001-f4eb-2168cd3e1994"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pdf_document(file):\n",
        "  print(f'Loading {file}')\n",
        "  loader=PyPDFLoader(file)\n",
        "  data = loader.load()\n",
        "  return data"
      ],
      "metadata": {
        "id": "QPsMPSqa8P4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#upload any file format\n",
        "\n",
        "def load_document(file):\n",
        "  import os\n",
        "  name,extension = os.path.splitext(file)\n",
        "  if extension =='.pdf':\n",
        "    data = load_pdf_document(file)\n",
        "  elif extension =='.docx':\n",
        "    from langchain.document_loaders import Docx2txtLoader\n",
        "    print(f'Loading {file}...')\n",
        "    loader = Docx2txtLoader(file)\n",
        "  else:\n",
        "    print('Document format is not supported')\n",
        "\n",
        "  data = loader.load()\n",
        "  return data\n"
      ],
      "metadata": {
        "id": "Mcx3Ut9x-uXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_from_wikipedia(query , lang='en'):\n",
        "  loader = WikipediaLoader(query=query, lang=lang, load_max_docs = 2) #limits the number fo dowloaded pages.\n",
        "  data=loader.load()\n",
        "  return data"
      ],
      "metadata": {
        "id": "htOcwm9_AWsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_data(data, chunk_size = 256):\n",
        "  from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size = chunk_size, chunk_overlap=0)\n",
        "  chunks= text_splitter.split_documents(data) #use split_documents when documents are already splitted by page\n",
        "  return chunks\n"
      ],
      "metadata": {
        "id": "Fy3drd_hGBh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "def print_embedding_cost(texts):\n",
        "  enc = tiktoken.encoding_for_model('text-embedding-ada-002')\n",
        "  total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
        "  print(f'Total Tokens : {total_tokens}')\n",
        "  print(f'Embedding Cost in USD: {total_tokens/1000*0.0004:.6f}')"
      ],
      "metadata": {
        "id": "yt0HyaXBG7UO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def insert_or_fetch_embeddings(index_name):\n",
        "  import pinecone\n",
        "  from langchain.vectorstores import Pinecone #Pinecone class\n",
        "  from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "  embeddings = OpenAIEmbeddings()\n",
        "  pinecone.init(api_key = os.environ.get('PINECONE_API_KEY'), environment=os.environ.get('PINECONE_env'))\n",
        "\n",
        "  if index_name in pinecone.list_indexes():\n",
        "    vector_store = Pinecone.from_existing_index(index_name, embeddings)\n",
        "  else:\n",
        "    print(f'Creating index {index_name} and embeddings...', end='')\n",
        "    pinecone.create_index(index_name, dimension =1536, metric = 'cosine')\n",
        "    vector_store=Pinecone.from_documents(chunks, embeddings, index_name=index_name)\n",
        "    print('Done!')\n",
        "  return vector_store"
      ],
      "metadata": {
        "id": "5gBrBJmJHvKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def del_pinecone_index(index_name='all'):\n",
        "  import pinecone\n",
        "  pinecone.init(api_key = os.environ.get('PINECONE_API_KEY'), environment=os.environ.get('PINECONE_env'))\n",
        "  if index_name=='all':\n",
        "      indexes= pinecone.list_indexes()\n",
        "      print('Deleting all indexes...')\n",
        "      for index in indexes:\n",
        "        pinecone.delete_index(index)\n",
        "      print('Ok')\n",
        "  else:\n",
        "    print(f'Deleting {index_name}...' , end='')\n",
        "    pinecone.delete_index(index_name)\n",
        "    print('Ok')\n",
        "\n"
      ],
      "metadata": {
        "id": "U2mjJTu5I60f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_with_memory(vector_store , question , chat_history = []):\n",
        "  from langchain.chains import ConversationalRetrievalChain\n",
        "  from langchain.chat_models  import ChatOpenAI\n",
        "\n",
        "  llm = ChatOpenAI(temperature = 1)\n",
        "  retriever = vector_store.as_retriever(search_type = 'similarity', search_kwargs = ({'k':3}))\n",
        "  crc = ConversationalRetrievalChain.from_llm(llm , retriever)\n",
        "  result = crc({'question':question , 'chat_history':chat_history})\n",
        "  chat_history.append((question , result['answer']))\n",
        "\n",
        "  return result , chat_history\n"
      ],
      "metadata": {
        "id": "nV72FMb7zcfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running the file\n"
      ],
      "metadata": {
        "id": "ZugXn5vC8-xa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_mobile = load_pdf_document('/content/Lecture 2 - History of Mobile Robots - W23.pdf')\n",
        "print(data_mobile[1].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxyAaTf48-Pe",
        "outputId": "d6ac7cb9-32eb-4785-9a73-9eac354a138d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading /content/Lecture 2 - History of Mobile Robots - W23.pdf\n",
            "Lecture 2:  History of Mobile Robots 2800 BCE | Golden Tripods\n",
            "Thetis' silver feet took her to Hephaestus' house, \n",
            "A mansion the lame god had built himself... \n",
            "She found him at his bellows, glazed with sweat \n",
            "As he hurried to complete his latest project, \n",
            "Twenty cauldrons on tripods to line his hall \n",
            "With golden wheels at the base of each tripod. \n",
            "....He was getting these ready, \n",
            "Forging the rivets with inspired artistry.\n",
            "-Homer, Iliad, 18:398, 8th century BC (Lombardo translation)           \n",
            "first concept for a mobile, multi -\n",
            "robot, autonomous, wheeled, \n",
            "delivery application robot team\n",
            "•Hephaestus is the Greek \n",
            "god of blacksmiths and \n",
            "metalworking. In the \n",
            "Illiad , he creates wheeled \n",
            "tripods to help him.\n",
            "Mount Olympus depiction on \n",
            "early pottery\n",
            " Tripods\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[10].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VSrXRB0-D-h",
        "outputId": "a7fc7470-f2df-4ab7-b2f5-89c018835851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lecture 2:  History of Mobile RobotsProbabilistic Computationalism |Bayesian Inference\n",
            "Thomas BayesProbabilities as Gaussian PDFs\n",
            "Probabilities as Sample Sets•Principled fusion of probability \n",
            "distributions from multiple \n",
            "sources\n",
            "•Priors over states\n",
            "•Probabilistic models for \n",
            "measurements\n",
            "•Probabilistic models for \n",
            "evolution of the robot state \n",
            "and environment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'The document has {len(data)} pages')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1A0vxWOf-ZnT",
        "outputId": "7c27128c-9312-4b3b-e6db-489559c48bbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The document has 49 pages\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_from_wikipedia('GPT-4')\n",
        "print(data[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3CaiuamA5OY",
        "outputId": "601f02b3-5fd9-4294-c8ac-4ad405251284"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generative Pre-trained Transformer 4 (GPT-4) is a multimodal large language model created by OpenAI, and the fourth in its series of GPT foundation models. It was initially released on March 14, 2023, and has been made publicly available via the paid chatbot product ChatGPT Plus, and via OpenAI's API.  As a transformer-based model, GPT-4 uses a paradigm where pre-training using both public data and \"data licensed from third-party providers\" is used to predict the next token. After this step, the model was then fine-tuned with reinforcement learning feedback from humans and AI for human alignment and policy compliance.: 2 Observers reported that the iteration of ChatGPT using GPT-4 was an improvement on the previous iteration based on GPT-3.5, with the caveat that GPT-4 retains some of the problems with earlier revisions. GPT-4 is also capable of taking images as input on ChatGPT. OpenAI has declined to reveal various technical details and statistics about GPT-4, such as the precise size of the model.\n",
            "\n",
            "\n",
            "== Background ==\n",
            " \n",
            "OpenAI introduced the first GPT model (GPT-1) in 2018, publishing a paper called \"Improving Language Understanding by Generative Pre-Training.\" It was based on the transformer architecture and trained on a large corpus of books. The next year, they introduced GPT-2, a larger model that could generate coherent text. In 2020, they introduced GPT-3, a model with 100 times as many parameters as GPT-2, that could perform various tasks with few examples. GPT-3 was further improved into GPT-3.5, which was used to create the chatbot product ChatGPT.\n",
            "Rumors claim that GPT-4 has 1.76 trillion parameters, which was first estimated by the speed it was running and by George Hotz.\n",
            "\n",
            "\n",
            "== Capabilities ==\n",
            "OpenAI stated that GPT-4 is \"more reliable, creative, and able to handle much more nuanced instructions than GPT-3.5.\" They produced two versions of GPT-4, with context windows of 8,192 and 32,768 tokens, a significant improvement over GPT-3.5 and GPT-3, which were limited to 4,096 and 2,049 tokens respectively. Some of the capabilities of GPT-4 were predicted by OpenAI before training it, although other capabilities remained hard to predict due to breaks in downstream scaling laws. Unlike its predecessors, GPT-4 is a multimodal model: it can take images as well as text as input; this gives it the ability to describe the humor in unusual images, summarize text from screenshots, and answer exam questions that contain diagrams.To gain further control over GPT-4, OpenAI introduced the \"system message\", a directive in natural language given to GPT-4 in order to specify its tone of voice and task. For example, the system message can instruct the model to \"be a Shakespearean pirate\", in which case it will respond in rhyming, Shakespearean prose, or request it to \"always write the output of [its] response in JSON\", in which case the model will do so, adding keys and values as it sees fit to match the structure of its reply. In the examples provided by OpenAI, GPT-4 refused to deviate from its system message despite requests to do otherwise by the user during the conversation.When instructed to do so, GPT-4 can interact with external interfaces. For example, the model could be instructed to enclose a query within <search></search> tags to perform a web search, the result of which would be inserted into the model's prompt to allow it to form a response. This allows the model to perform tasks beyond its normal text-prediction capabilities, such as using APIs, generating images, and accessing and summarizing webpages.A 2023 article in Nature stated programmers have found GPT-4 useful for assisting in coding tasks (despite its propensity for error), such as finding errors in existing code and suggesting optimizations to improve performance. The article quoted a biophysicist who found that the time he required to port one of his programs from MATLAB to Python went down from days to \"an hour or so\". On a test of 89 security scenarios, GPT-4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Chunking strategies -  a chunk of text should make sense without the other content\n",
        "\n",
        "chunks = chunk_data(data_mobile)\n",
        "print(f'We have {len(chunks)} chunks')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yb3OA2crBEqn",
        "outputId": "55aaeb96-33d3-4170-a6c5-09a75a45c799"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have 124 chunks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chunks[3].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIex5AABG4Gb",
        "outputId": "9e02678d-422b-40d8-f86e-32fac07afad7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Twenty cauldrons on tripods to line his hall \n",
            "With golden wheels at the base of each tripod. \n",
            "....He was getting these ready, \n",
            "Forging the rivets with inspired artistry.\n",
            "-Homer, Iliad, 18:398, 8th century BC (Lombardo translation)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_embedding_cost(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZteI2ukmHS5l",
        "outputId": "40de11ff-7d40-4b9d-d47e-3c39b4614705"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Tokens : 6094\n",
            "Embedding Cost in USD: 0.002438\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#embedding and uplaoding chunks to pinecone databse\n",
        "del_pinecone_index()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19Xw5DgYHceo",
        "outputId": "835f2d12-c163-4272-8e28-adf1b36ee5e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleting all indexes...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_name = 'askaquestion'\n",
        "vector_store = insert_or_fetch_embeddings(index_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvsolAKYJ0OB",
        "outputId": "0d7e0bb4-c241-4632-eac5-8d31ab4ed655"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating index askaquestion and embeddings...Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Query the database\n",
        "\n",
        "def QnA(vector_store , q):\n",
        "  from langchain.chains import RetrievalQA\n",
        "  from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "  llm=ChatOpenAI(model='gpt-3.5-turbo', temperature = 1)\n",
        "  retriever = vector_store.as_retriever(search_type = 'similarity', search_kwargs = ({'k':3}))\n",
        "  chain = RetrievalQA.from_chain_type(llm=llm, chain_type='stuff',retriever = retriever)\n",
        "  answer = chain.run(q)\n",
        "  return answer"
      ],
      "metadata": {
        "id": "I4tdPgXgKVqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q = 'What is the whole document about?'\n",
        "answer = QnA(vector_store , q)\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6xJtPvrOa99",
        "outputId": "8f90c360-5c47-46ec-a21e-9c20bf77383d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The document is likely about the history and advancements in the field of mobile robots, including specific examples such as the Braitenberg Vehicles and the DARPA Urban Grand Challenge. It may also discuss different approaches to mobile robot behavior, such as finite state machines.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "i=1\n",
        "print('Write Quit or Exit to quit.')\n",
        "while True:\n",
        "  q=input(f'Question #{i}: ')\n",
        "  i=i+1\n",
        "  if q.lower() in ['quit' , 'exit']:\n",
        "    print('Bye Bye!')\n",
        "    time.sleep(2)\n",
        "    break\n",
        "  answer = QnA(vector_store,q)\n",
        "  print(f'\\nAnwer: {answer}')\n",
        "  print(f'\\n{\"-\"*50} \\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwGowRsvOl9I",
        "outputId": "fd0e5091-cb08-415c-d339-8f4fbc605d76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Write Quit or Exit to quit.\n",
            "Question #1: what is the document about?\n",
            "\n",
            "Anwer: The document appears to be about the history of mobile robots, specifically mentioning the DARPA Urban Grand Challenge in 2007 and the Braitenberg Vehicles. It also discusses various milestones and achievements in the field of mobile robotics, such as the first outdoor mobile robot and the first mobile robot on another planetary surface.\n",
            "\n",
            "-------------------------------------------------- \n",
            "\n",
            "Question #2: what are Braitenberg Vehicles?\n",
            "\n",
            "Anwer: Braitenberg Vehicles are a concept introduced by Valentin von Braitenberg in his book \"Vehicles: Experiments in Synthetic Psychology\". They are hypothetical vehicles or machines that exhibit simple behaviors based on their sensor and motor connections. Braitenberg describes 14 different vehicles in his book, each with increasingly complex layers of behavior. These vehicles demonstrate various phenomena such as foresight, intentionality, individuation, egoism, and optimism.\n",
            "\n",
            "-------------------------------------------------- \n",
            "\n",
            "Question #3: what is special about them?\n",
            "\n",
            "Anwer: The tortoises built by neurologist Grey Walter in 1948-9 were special because they were able to avoid obstacles based on touch sensors.\n",
            "\n",
            "-------------------------------------------------- \n",
            "\n",
            "Question #4: quit\n",
            "Bye Bye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def QnA_w_history(vector_store , q, chat_history =[]):\n",
        "  from langchain.chains import ConversationalRetrievalChain\n",
        "  from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "  llm=ChatOpenAI(model='gpt-3.5-turbo', temperature = 1)\n",
        "  retriever = vector_store.as_retriever(search_type = 'similarity', search_kwargs = ({'k':3}))\n",
        "  chain_conv = ConversationalRetrievalChain.from_llm(llm=llm,retriever = retriever)\n",
        "  result = chain_conv({'question': q , 'chat_history': chat_history})\n",
        "  chat_history.append((q , result['answer']))\n",
        "  return result , chat_history"
      ],
      "metadata": {
        "id": "b4NMM4efQTBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history=[]\n",
        "q='What are mobile robotics'\n",
        "result = QnA_w_history(vector_store , q, chat_history= [])\n",
        "print(result['answer'])\n",
        "print(chat_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "6JZxIXmSVkbA",
        "outputId": "1662e02c-91b4-4c97-c34a-5a41f97d14ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-af6438b776d5>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'What are mobile robotics'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQnA_w_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_store\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchat_history\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'answer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchat_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#asking with chat history\n",
        "chat_history = []\n",
        "question =  'What are mobile robotics?'\n",
        "result , chat_history = ask_with_memory(vector_store , question , chat_history = [])\n",
        "print(result['answer'])\n",
        "print(chat_history)"
      ],
      "metadata": {
        "id": "3LhxZk8dzDwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question2 = '....'\n",
        "result , chat_history = ask_with_memory(vector_store , question2 , chat_history = [])\n",
        "print(result['answer'])\n",
        "print(chat_history)"
      ],
      "metadata": {
        "id": "gywJmoMEV1yY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}